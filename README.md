# Machine Learning Homework 2: Neural Network Exploration

This machine learning project was undertaken using Python and Keras, focusing on neural networks. The exercise explored network architectures and activation functions, and the capability to reproduce results was demonstrated.

## Highlights

- **Implementation**: Python and the Keras library were used for the exercises.

- **Backpropagation**: The backpropagation algorithm for Multi-Layer Perceptrons (MLPs) was revisited, and adjustments were made for:
  - ReLU (Rectified Linear Unit)
  - Hyperbolic Tangent
  - Sigmoid

- **MNIST Classification**: The MNIST dataset was used to address handwritten digit recognition. The approach included:
  - **Data Preprocessing**: Data was normalised.
  - **Network Depth**: Networks of 5, 20, and 40 layers were experimented with.
  - **Activation Functions**: Performance was assessed with ReLU, Hyperbolic Tangent, and Sigmoid.
  - **Compilation**: The SGD optimizer with a learning rate of 0.01 was used, and the categorical crossentropy loss function was employed.

- **Adaptability**: The capability to utilize any publicly available dataset was shown.

- **Reproducibility**: Documentation and code clarity were maintained for easy replication.
